{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "_Author: Dhavide Aruliah_\n",
    "\n",
    "### Assignment Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "- [Question 1: Implementing a Hard Threshold Activation Function](#q-threshold)\n",
    "- [Question 2: Implementing the Perceptron Classifier](#q-classifier)\n",
    "- [Question 3: Identifying Misclassified Points](#q-misclassified)\n",
    "- [Question 4: Building the actual Perceptron Iteration](#q-iteration)\n",
    "- [Question 5: Implementing `softmax`](#q-softmax)\n",
    "- [Question 6: Implementing `relu`](#q-relu)\n",
    "- [Question 7: Preprocessing the Digit Features](#q-features)\n",
    "- [Question 8: Preprocessing the Targets](#q-targets)\n",
    "- [Question 9: Setting up the Keras Architecture](#q-architecture)\n",
    "- [Question 10: Fitting the Neural Network to Training Data](#q-fitting)\n",
    "- [Question 11: Assessing Neural Network Model Accuracy](#q-assessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "This assignment provides an opportunity to get used to *artificial neural networks* for supervised machine learning. We'll do this first by looking at the *perceptron* algorithm as an early example of a neural network. By building a simple program implementing the perceptron, you'll get a sense of the mathematical ideas underlying the training if neural networks. From there, you'll experiment with [Keras](https://keras.io/) as an example of a framework for building neural networks and solve a simple classification problem.\n",
    "\n",
    "The goals of the present assignment are:\n",
    "+ to get used to the core terminology used with neural networks: layers, units, activation functions, etc.\n",
    "+ to implement the simplest neural network algorithm (the perceptron) to build a conceptual foundation on which neural networks are built.\n",
    "+ to solve a simple problem (digit classification) using a neural networks framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Standard boilerplate for Python for data science\n",
    "%matplotlib inline\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## The Perceptron\n",
    "\n",
    "To begin, you will replicate one of the first models of an artificial Neural Network that came from [Frank Rosenblatt](https://en.wikipedia.org/wiki/Frank_Rosenblatt) in 1957. While working on research funded by the US Defense Department, Rosenblatt investigated a straightforward approach to developing a classification model. You will construct a basic implementation of the Perceptron from scratch before moving to the Keras library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)\n",
    "\n",
    "---\n",
    "<a id=\"q-threshold\"></a>\n",
    "## Question 1: Implementing a Hard Threshold Activation Function\n",
    "\n",
    "Implement the hard threshold activation function `sign` with function signature `sign(t)` as given below.\n",
    "+ Your function should follow the convention\n",
    "  $$\\mathrm{sign}(t) = \\begin{cases} +1, & t\\ge0 \\\\ -1, &t<0 \\end{cases}$$\n",
    "  for any real value $t\\in\\mathbb{R}$.\n",
    "+ Make sure your function `sign` is [*vectorized*](https://docs.scipy.org/doc/numpy/glossary.html#term-vectorization) (i.e., is a [*universal function*](https://docs.scipy.org/doc/numpy/reference/ufuncs.html) in the parlance of Numpy). That is, it should accept a Numpy array as input and return a Numpy array of identical dimensions with entries $+1$ or $-1$ as required (i.e., the $\\text{sign}$ function should be applied elementwise to the array).\n",
    "+ Notice that `np.sign` won't work here (because `np.sign(0)==0` and you want `sign(0)==+1` instead).\n",
    "+ The function `np.where` is likely useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "Q-01",
     "locked": false,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### QUESTION 1\n",
    "### Complete the body of the function sign (with signature below) in agreement with\n",
    "###   the details specified above.\n",
    "\n",
    "def sign(t):\n",
    "    return np.where(t>=0, 1, -1) # syntax: np.where(x,y,z)\n",
    "                                 # where t>=0 then 1, else -1\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Q-01",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)\n",
    "\n",
    "---\n",
    "<a id=\"q-classifier\"></a>\n",
    "\n",
    "## Question 2: Implementing the Perceptron Classifier\n",
    "\n",
    "Implement the function $f_{\\mathrm{perceptron}}$ as defined in the video 20-01 with the signature `f_perceptrion(X, w, b)`. Mathematically, it could be written as $f_{\\mathrm{perceptron}}(x) = \\mathrm{sign}(W x^T +b)$ where $x$ is a row vector (i.e., one-dimensional array) of length $d$, $W$ is a row vector of length $d$, and $b$ is a scalar. The preceding equation differs slightly from the videos in assuming that $x$ is a row vector.\n",
    "+ You will make the function more flexible by allowing for $N\\times d$ *feature matrices* $X$ as input. In that case, the function can be computed as $f_{\\mathrm{perceptron}}(x) = \\mathrm{sign}(W X^T + b)$ (in which case the output is a $1\\times N$ vector rather than a $1\\times 1$ scalar).\n",
    "+ Tip: If the conventions around row & column vectors are messy, consider using `np.squeeze` to reduce two-dimensional row or column vectors to one-dimensional vectors. Numpy is very permissive about computing matrix-vector products using one-dimensional arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function is used when taking two seperate groups of data points classified by -1 and +1, then using linear regression to draw a line between the two groups after being plotted on a graph'\n",
    "\n",
    "    'Returns sign(W X^T + b) or the \"activation function\"'\n",
    "    'd = length'\n",
    "    'x is the row vector of length d(1 x d, or 1d or one dimensional)'\n",
    "    'X is the matrix(N x d, or Nd, or N dimensional)\n",
    "    'W is the row vector of length d'\n",
    "    'b or \"Bias Term\" is the scalar/type of 1 x 1 matrix'\n",
    "    'In the video x is not a single row vector but it is in this example'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### QUESTION 2\n",
    "### Complete the function f_perceptron as specified above.\n",
    "###\n",
    "\n",
    "def f_perceptron(X, W, b):\n",
    "    'Returns sign(W X^T + b) or the \"activation function\"' \n",
    "    no_bias = np.dot(W,(np.transpose(X)))\n",
    "    return np.sign(no_bias + b)\n",
    "\n",
    "    \n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Q-02",
     "locked": true,
     "points": "8",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)\n",
    "\n",
    "---\n",
    "<a id=\"q-misclassified\"></a>\n",
    "\n",
    "## Question 3: Identifying Misclassified Points\n",
    "\n",
    "Your task now is to write a function `find_misclassified` that identifies points that are misclassified by `f_perceptron` from above.\n",
    "+ The function signature is `find_misclassified(X, y, W, b)` with:\n",
    "  + $N\\times d$ *feature matrix* `X`;\n",
    "  + *weight vector* `W` of length $d$;\n",
    "  + (scalar) *bias* `b`; and\n",
    "  + target vector `y` of length $N$ with entries $+1$ or $-1$.\n",
    "  The inputs `X`, `W`, and `b` are exactly as required for evaluating `f_perceptron`.\n",
    "+ The output computed by the function `find_misclassified` is a one-dimensional Numpy array or a list of integers corresponding to rows of the input `X` that are misclassified by $f_{\\mathrm{perceptron}}$. That is, `find_misclassified` returns the rows $k$ between $0$ and $N-1$ for which\n",
    "  $$f_{\\mathrm{perceptron}}(X_{k,:}, W, b) \\neq y_{k},$$\n",
    "  where $X_{k,:}$ refers to the $k$th row of the $N\\times d$ matrix $X$.\n",
    "+ The row indices output by `find_misclassified` should be sorted in increasing order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### QUESTION 3\n",
    "### Complete the function find_misclassified as specified above.\n",
    "###\n",
    "def find_misclassified(X, y, W, b):\n",
    "    ###'Returns 1D array of index values for which f_perceptron misclassifies rows of X'\n",
    "    \n",
    "    N, d = X.shape\n",
    "    rows = np.arange(N)\n",
    "    y_hat = np.squeeze(f_perceptron(X, W, b))\n",
    "    M = rows[y_hat != np.squeeze(y)]\n",
    "    N = np.sort(M)\n",
    "    return np.where(M != N, N, M)\n",
    "    \n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX = np.array([[  4.20811019,  -0.09798517,   4.75826135,  -6.23588559], [  3.14467235,  -8.0575068 ,   0.24582028,   1.05472419],\n",
    "       [ -2.95200329,  -0.47806581,   0.43846259,   0.46692371], [  4.12273971,  -5.46557231,  -2.40378627,  -8.86015579]])\n",
    "testy = np.array([ 1., 1., 1., -1.])\n",
    "Wtest, btest = np.random.randn(testX.shape[1]), np.random.randint(low=-5, high=+5, size=(1,))\n",
    "\n",
    "find_misclassified(testX, testy, Wtest, btest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Q-03",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given training data $X$ & $y$ for a binary classification problem, you are now ready to implement the perceptron algorithm to determine a classifier with parameters $W$ and $b$. The basic steps are:\n",
    "\n",
    "+ Fix a random seed for the iteration (optional, but useful for reproducibility)\n",
    "+ Initialize $W$ and $b$ with some random values;\n",
    "+ REPEAT:\n",
    "  + Compute the set $\\mathcal{M}$ of rows of $X$ misclassified by $f_{\\mathrm{perceptron}}(\\cdot , W, b)$\n",
    "  + Draw a row index $k$ from $\\mathcal{M}$ at random\n",
    "  + Use the $k$th row of $X$ and the $k$th entry of the label vector $y$ to update $W$ and $b$:\n",
    "    $$\\begin{aligned} W &\\leftarrow W + \\eta y_{k} X_{k,:} \\\\ b &\\leftarrow b + \\eta y_{k} \\end{aligned}$$\n",
    "    (above $\\eta$ is a user-specified *learning rate*).\n",
    "+ UNTIL convergence (i.e., until $\\mathcal{M}$ is empty or, optionally, if the number of iterations is too large).\n",
    "\n",
    "Your next task is to write a function that carries out this iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)\n",
    "\n",
    "---\n",
    "<a id=\"q-iteration\"></a>\n",
    "## Question 4: Building the actual Perceptron Iteration\n",
    "\n",
    "+ The function `perceptron_iteration` with the signature `perceptron_iteration(X, y, eta=1.0, ITMAX=1000, random_state=None)`.\n",
    "  + the positional inputs `X` and `y` are exactly as before from `find_misclassified`.\n",
    "  + the optional keyword argument `eta` is the *learning rate* (default value `1.0`).\n",
    "  + the optional keyword argument `ITMAX` is the maximum number of iterations (default value `1000`).\n",
    "  + the optional keyword argument `random_state` is for seeding random number generation (default value `None`).\n",
    "+ Convergence is achieved when the set $\\mathcal{M}$ of row indices of misclassified points is *empty*.\n",
    "+ The function should return the weights $W$ and the bias term $b$ as a 2-tuple on convergence.\n",
    "+ If the iteration fails to converge within `ITMAX` iterations (i.e., if there are still points being misclassified), the function should return `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### QUESTION 4\n",
    "### Complete the function perceptron_iteration as laid out below.\n",
    "###\n",
    "def perceptron_iteration(X, y, eta=1.0, ITMAX=1000, random_state=None):\n",
    "    '''Applies the perceptron algorithm to compute weights W and bias b associated with a binary\n",
    "    classification problem defined by N by d feature matrix X and N-vector y of labels.\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(seed=random_state) # DO NOT CHANGE THIS LINE\n",
    "    N, d = X.shape\n",
    "    W, b = np.random.randn(d), np.random.randn(1)\n",
    "    \n",
    "   # Determine misclassified rows of X\n",
    "    M = find_misclassified(X, y, W, b)\n",
    "    \n",
    "    # ITERATE:\n",
    "    # Choose an index k from misclassified rows\n",
    "    for iteration in range(ITMAX):\n",
    "        \n",
    "        # Compute updates to W and b\n",
    "        if len(M):\n",
    "            k = np.random.choice(M)\n",
    "            W, b = W + eta * y[k] * X[k,:], b + eta*y[k]\n",
    "            M = find_misclassified(X, y, W, b)\n",
    "        else:\n",
    "            break\n",
    "        # Determine misclassified rows of X with new W and b\n",
    "        # new_misclassified = find_misclassified(X, y, updates_W, updates_b)\n",
    "        # Return W & b (unless convergence failed, so return (None, None))\n",
    "    if iteration < (ITMAX-1):\n",
    "        return W, b\n",
    "    else:\n",
    "        return (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Q-04",
     "locked": true,
     "points": "20",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Neural networks\n",
    "\n",
    "Recognizing that the perceptron has two conceptual layers (an *input* layer and an *output* layer) that connects spaces of disparate dimensions, we can extend this model to more general functions. That is, we can build a *multi-layer perceptron* with multiple input layers, each associated with their own weight matrix, bias vector, and activation function. It is possible to use more general *network architectures* with these components to represent more sophisticated functions.\n",
    "\n",
    "A key component of this is the choice of *activation function*.\n",
    "\n",
    "+ For binary classification problems, the logistic activation function\n",
    "  $$ \\sigma(t) = \\frac{e^{t}}{1+e^t} $$\n",
    "  (as seen from logisitc regression) is useful for probabilities of belonging to one of the two classes.\n",
    "+ For multiclass classification problems, the $\\text{softmax}$ function is often used to provide probabilities of belonging to any of a number of classes. It is a mapping $x \\mapsto \\mathrm{softmax}(x)$ of a vector of length $d$ to another vector of length $d$ defined by\n",
    "  $$[\\mathrm{softmax}(x)]_{k} := \\frac{e^{x_k}}{\\sum_{i=1}^{d}e^{x_{i}}} = \\frac{\\exp(x_k)}{\\sum_{i=1}^{d} \\exp(x_i)} \\qquad(k=1,2,\\dotsc,d).$$\n",
    "  Notice that the nonnegative entries of $\\mathrm{softmax}(x)$ add up to $1$ so it is, in effect, a discrete probability mass function.\n",
    "+ The *ReLU* (\"rectified linear unit\") function is a piecewise linear function defined by\n",
    "  $$\\mathrm{relu}(t) = \\begin{cases} t, & t\\ge0 \\\\ 0, &t<0 \\end{cases}.$$\n",
    "  The ReLU function is often used in between internal layers of a regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)\n",
    "\n",
    "---\n",
    "<a id=\"q-softmax\"></a>\n",
    "## Question 5: Implementing `softmax`\n",
    "\n",
    "For this task, you will complete the Python function `softmax` function defined by\n",
    "$$[\\mathrm{softmax}(x)]_{k} := \\frac{e^{x_k}}{\\sum_{i=1}^{d}e^{x_{i}}} = \\frac{\\exp(x_k)}{\\sum_{i=1}^{d} \\exp(x_i)}.$$\n",
    "+ To obtain a more numerically robust implementation, do the following: if $M=\\max_{i}(x_{i})$, then\n",
    "  $$[\\mathrm{softmax}(x)]_{k} = \\frac{\\exp(x_k - M)}{\\sum_{i=1}^{d} \\exp(x_i-M)}.$$\n",
    "  You will complete the function `softmax` using this version (which does not overflow for large values of $x_k$).\n",
    "+ Make sure your function is *vectorized* (i.e., is a *universal function* in the parlance of Numpy). That is, it should accept a Numpy array as input and return a Numpy array of identical dimensions with appropriate real-valued entries required (i.e., the $\\mathrm{softmax}$ function should be applied elementwise to the array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e_x:  [1.         0.13533528 0.06081006] | x:  [3.0, 1.0, 0.2] | max(x):  3.0\n",
      "[0.8360188  0.11314284 0.05083836]\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### QUESTION 5\n",
    "### Complete the function softmax as defined above.\n",
    "###\n",
    "def softmax(x):\n",
    "    'Returns smoothed version of max. function'\n",
    "    e_x = np.exp(x - np.max(x)) # np.exp calcs exponential of all array values\n",
    "                                # np.max returns the max value along a given axis of an arrary\n",
    "    print(\"e_x: \",e_x,\"|\",\"x: \",x,\"|\",\"max(x): \",np.max(x)) \n",
    "                                # So, the equation takes an array 'x' and subtracts the max value of that array then calcs expo \n",
    "    return e_x / e_x.sum()      # then divides those exponential values by their sum (i.e. - Softmax)\n",
    "\n",
    "scores = [3.0, 1.0, 0.2]\n",
    "print(softmax(scores))\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Q-05",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)\n",
    "\n",
    "---\n",
    "<a id=\"q-relu\"></a>\n",
    "## Question 6: Implementing `relu`\n",
    "\n",
    "You task here is to complete the function `relu` (the *rectified linear unit* or \"ReLU\" function) with signature given below.\n",
    "+ Your function `relu` should follow the convention\n",
    "  $$\\mathtt{relu}(t) = \\begin{cases} t, & t\\ge0 \\\\ 0, &t<0 \\end{cases}$$\n",
    "  for any real value $t\\in\\mathbb{R}$.\n",
    "+ Make sure your function is *vectorized* (i.e., is a *universal function* in the parlance of Numpy). That is, it should accept a Numpy array as input and return a Numpy array of identical dimensions with appropriate real-valued entries required (i.e., the $\\mathrm{relu}$ function should be applied elementwise to the array).\n",
    "+ The function `np.where` is likely useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### QUESTION 6\n",
    "### Complete the function relu as specified above.\n",
    "###\n",
    "def relu(t):\n",
    "    \"Returns t for t>=0, zero otherwise\"\n",
    "    return np.where(t>=0, t, 0)\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Q-06",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Introduction to Keras\n",
    "\n",
    "<center>\n",
    "    <a href=\"http://keras.io\"><img src = \"assets/keras.png\" width = \"50%\" height = \"50%\" /></a>\n",
    "</center>\n",
    "\n",
    "Now that you have some familiarity with activation functions and with the perceptron as a simple neural network, you can use [Keras](http://keras.io) as a framework to solve problems using neural networks. Keras is a library that provides a simple API for neural network algorithms on top of lower-level libraries like [Tensorflow](https://www.tensorflow.org/) or [Theano](https://github.com/Theano/Theano). Other high-level frameworks for neural networks (\"deep learning\") include [Chainer](https://chainer.org/) and [PyTorch](https://pytorch.org/).\n",
    "\n",
    "In the next few exercises, you'll get a chance to solve the [MNIST handwritten digits classification problem](http://yann.lecun.com/exdb/mnist/) using Keras. The package provides [utilities to work with numerous datasets](https://keras.io/datasets/), but we'll avoid those here (to limit needless network traffic). Instead, we've created a compressed Numpy file in the local `assets` folder that contains the relevant data. The next few lines extract the arrays `X_train`, `y_train`, `X_test`, and `y_test` from that file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "d_file = np.load('assets/mnist.npz')\n",
    "X_train_orig = d_file['X_train']\n",
    "X_test_orig = d_file['X_test']\n",
    "y_train_orig = d_file['y_train']\n",
    "y_test_orig = d_file['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) 47040000\n",
      "(10000, 28, 28) 7840000\n",
      "(60000,) 60000\n",
      "(10000,) 10000\n"
     ]
    }
   ],
   "source": [
    "for arr in [X_train_orig, X_test_orig, y_train_orig, y_test_orig]:\n",
    "    print(arr.shape, arr.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_orig[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract a single image\n",
    "idx = 45621\n",
    "digit_image = X_train_orig[idx]\n",
    "plt.imshow(digit_image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Associated label: {}'.format(y_train_orig[idx]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)\n",
    "\n",
    "---\n",
    "<a id=\"q-features\"></a>\n",
    "## Question 7: Preprocessing the Digit Features\n",
    "\n",
    "As a first step, preprocess the features in the arrays `X_train_orig` & `X_test_orig`.\n",
    "\n",
    "+ Reshape the three-dimensional arrays into two-dimensional arrays.\n",
    " + Numpy arrays have a method for reshaping (or use the function `np.reshape`).\n",
    "+ Rescale the integer values to be real values between 0 and 1.\n",
    " + Divide the arrays by 255.0 (the grayscale images have integer values between 0 & 255 by default).\n",
    "+ Bind the rescaled & reshaped training & testing arrays to `X_train` & `X_test` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (60000, 784)\n",
      "X_test:  (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### QUESTION 7\n",
    "### Rescale & reshape the feature arrays X_train_orig & X_test_orig as described above.\n",
    "### Assign the results to X_train & X_test respectively.\n",
    "### YOUR SOLUTION HERE:\n",
    "\n",
    "X_train = X_train_orig.reshape(X_train_orig.shape[0],-1) / 255.0\n",
    "X_test  = X_test_orig.reshape(X_test_orig.shape[0],-1) / 255.0\n",
    "\n",
    "# the shape[0], -1\n",
    "# the -1 in the reshape tells numpy to figure out the dimensions\n",
    "# the [0] in the reshape reduces the 3D array to a 2D array\n",
    "\n",
    "### For verification:\n",
    "print('X_train: {}'.format(X_train.shape))\n",
    "print('X_test:  {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Q-07",
     "locked": true,
     "points": "8",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)\n",
    "\n",
    "---\n",
    "<a id=\"q-targets\"></a>\n",
    "## Question 8: Preprocessing the Targets\n",
    "\n",
    "As a second step, preprocess the targets `y_train_orig` & `y_test_orig` by converting them to two-dimensional arrays with one-hot encoded rows (corresponding to each categorical label).\n",
    "+ The function `to_categorical` from `keras.utils` will do this for you.\n",
    "+ Bind the results to `y_train` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train: (60000, 10)\n",
      "y_test:  (10000, 10)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### QUESTION 8\n",
    "### Convert the target arrays y_train_orig & y_test_orig as described above.\n",
    "### Assign the results to y_train & y_test respectively.\n",
    "from keras.utils import to_categorical\n",
    "### YOUR SOLUTION HERE:\n",
    "y_train = to_categorical(y_train_orig)\n",
    "y_test = to_categorical(y_test_orig)\n",
    "### For verification:\n",
    "print('y_train: {}'.format(y_train.shape))\n",
    "print('y_test:  {}'.format(y_test.shape))\n",
    "print(y_test[:3]) # First three rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Q-08",
     "locked": true,
     "points": "7",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)\n",
    "\n",
    "---\n",
    "<a id=\"q-architecture\"></a>\n",
    "## Question 9: Setting up the Keras Architecture\n",
    "\n",
    "You are now ready to instantiate a neural network model to solve the digits classification problem. This is referred to as specifying the *architecture* of the neural network.\n",
    "\n",
    "+ To initialize the model, instantiate an object of the class `models.Sequential`. Use the default options.\n",
    "+ Add a hidden layer using `network.add` with `layers.Dense`.\n",
    "  + The first argument to `layers.Dense` is `512` (for 512 units).\n",
    "  + Use the keyword argument `activation='relu'` to specify the ReLU activation function for this layer.\n",
    "  + As this is the first layer instantiated, it is necessary to specify `input_shape` as a *tuple*.\n",
    "+ Add a final output layer using `network.add` & `layers.Dense`.\n",
    "  + This layer will have 10 units and `activation='softmax'` to specify the final output of the setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GRADED\n",
    "### QUESTION 9\n",
    "### Create the neural network architecture using models.Sequential.\n",
    "###   Bind the object to the identifier \"network\" and add two layers: a dense layer with\n",
    "###   512 units with an activation function 'relu' and another dense layer, this time with\n",
    "###  10 units and an activation function 'softmax'. Remember that the first layer requires\n",
    "###  specification of the input dimensions. \n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# Instantiate the model\n",
    "network = models.Sequential()\n",
    "\n",
    "# hidden layer\n",
    "network.add(layers.Dense(512, activation='relu', input_shape = X_train.shape[1:]))\n",
    "\n",
    "# output layer\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Q-09",
     "locked": true,
     "points": "12",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "To get `network` ready to fit to the training data, you have to first *compile* it. This involves specifying the optimizer (a choice of strategies to apply to solve for the network parameters), the loss function to minimize (categorical cross-entropy in this case as is common for multi-class classification problems), and a choice of metrics to track in the iterative process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)\n",
    "\n",
    "---\n",
    "<a id=\"q-fitting\"></a>\n",
    "## Question 10: Fitting the Neural Network to Training Data\n",
    "\n",
    "You can fit the training data and evaluating prediction using the `fit` method.\n",
    "+ The `fit` method requires the feature array `X_train` and the target array `y_train` as inputs.\n",
    "+ As optional keyword arguments, specify `epochs=5` (the number of sweeps through the data to make) and `batch_size=128` (the number of data points to use in each sweep through the data). This is in principle the same as the iterations of stochastic gradient descent (with a batch size of 1) made in the perceptron algorithm.\n",
    "+ Bind the output of `network.fit` to the identifier `history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.2576 - acc: 0.9249\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.1020 - acc: 0.9697\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0681 - acc: 0.9797\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0494 - acc: 0.9849\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0374 - acc: 0.9887\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### QUESTION 10\n",
    "### Apply the fit method to the object network to using the training data and the options\n",
    "###   specified above. Assign the result to the identifier history.\n",
    "### YOUR SOLUTION HERE:\n",
    "\n",
    "history = network.fit(X_train, y_train, epochs = 5, batch_size = 128) # fit the model similar to scikit learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Q-10",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Assignment-Contents)\n",
    "\n",
    "---\n",
    "<a id=\"q-assessing\"></a>\n",
    "## Question 11: Assessing Neural Network Model Accuracy\n",
    "\n",
    "Finally, you can assess the accuracy of the .model using the testing data.\n",
    "+ Apply the `evaluate` method to the object `network` with the testing data `X_test` & `y_test` as input.\n",
    "+ The output will be a sequence of two values: the loss and the accuracy. Assign these values to `test_loss` and `test_acc` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 54us/step\n",
      "test_loss:   0.07164\n",
      "test_acc:     0.9789\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "### QUESTION 11\n",
    "### Use the testing arrays X_test & y_test to compute the loss function & accuracy on the test data.\n",
    "###   Assign the results to test_loss & test_acc respectively.\n",
    "### YOUR SOLUTION HERE:\n",
    "test_loss, test_acc = network.evaluate(X_test, y_test)\n",
    "print('test_loss: {:9.4g}'.format(test_loss))\n",
    "print('test_acc:  {:9.4g}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Q-11",
     "locked": true,
     "points": "5",
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Now you've worked through this, you've built your first neural network model from end-to-end!\n",
    "\n",
    "[Back to top](#Assignment-Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## References\n",
    "\n",
    "+ [*Artificial Neural Network* ](https://en.wikipedia.org/wiki/Artificial_neural_network) (Wikipedia)\n",
    "+ [Neural Networks & Deep Learning](http://neuralnetworksanddeeplearning.com/) by Michael Nielsen\n",
    "+ [Keras documentation](https://keras.io)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
